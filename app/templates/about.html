<!DOCTYPE html>
<html lang="en">
	<head>
		<title>About - SWE-Readers</title>
		<meta charset="utf-8">

		<!-- pulling Bootstrap from CDN -->
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

		<!-- Web CSS Stylesheet -->
		<link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/swe-readers-stylesheet.css') }}">
		
		<!-- Local CSS Stylesheet -->
		<!-- <link rel="stylesheet" type="text/css" href="swe-readers-stylesheet.css"> -->
	
	</head>
	<body>
		<nav class="navbar navbar-inverse"></nav>
		<nav class="navbar navbar-inverse navbar-fixed-top">
			<div class="container-fluid">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
						<span class="glyphicon glyphicon-book white"></span>
					</button>
					<a class="navbar-brand" href="{{ url_for('home') }}">SWE-Readers</a>
				</div>
				<div class="collapse navbar-collapse" id="myNavbar">
					<ul class="nav navbar-nav">
						<li><a href="{{ url_for('books') }}">Books</a></li>
						<li><a href="{{ url_for('authors') }}">Authors</a></li>
						<li><a href="{{ url_for('publishers') }}">Publishers</a></li>
						<li><a href="{{ url_for('reviews') }}">Reviews</a></li>
						<li class="active"><a href="{{ url_for('about') }}">About</a></li>
					</ul>
					<form class="navbar-form navbar-right" action="{{ url_for('search') }}">
						<div class="input-group">
							<input type="text" class="form-control" placeholder="Search">
							<div class="input-group-btn">
								<button class="btn btn-default" type="submit">
									<i class="glyphicon glyphicon-search"></i>
								</button>
							</div>
						</div>
					</form>
				</div>
			</div>
		</nav>
		<div class="container">
			<h1>About us!</h1>
			<h4>Meet the team behind SWE-Readers:</h4>
		</div>
		<div class="container">
			<div class="row">
				<div class="col-sm-6 col-md-3">
					<div class="card">
						<div class="card-block">
							<img class="card-img-top img-responsive" src="{{ url_for('static', filename='img/about/nando.jpg') }}" alt="Card image cap">
							<h4 class="card-title">Fernando Mendoza</h4>
							<p class="card-text">
								Bio: I'm a senior CS student at the Univerity 
								of Texas at Austin. I play and record my own 
								music.
							</p>
							<p class="card-text">
								Responsibilites: Backend, Webmaster
							</p>
							<p class="card-text">Commits: 160</p>
							<p class="card-text">Issues: 13</p>
							<p id ="cardbottom" class="card-text">Unit Tests:6</p>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-md-3">
					<div class="card">
						<div class="card-block">
							<img class="card-img-top img-responsive" src="{{ url_for('static', filename='img/about/Dylan.jpg') }}" alt="Card image cap">
							<h4 class="card-title">Dylan Droll</h4>
							<p class="card-text">
								Bio: I'm a junior CS student at the University 
								of Texas. My hobbies include but are not 
								limited to: Video games, writing, watching 
								movies, and cooking. One day I want to own a 
								corgi ranch and write stories.
							</p>
							<p class="card-text">
								Responsibilites: GUI, Web Design
							</p>
							<p class="card-text">Commits: 142</p>
							<p class="card-text">Issues: 15</p>
							<p id ="cardbottom" class="card-text">Unit Tests:0</p>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-md-3">
					<div class="card">
						<div class="card-block">
							<img class="card-img-top img-responsive" src="{{ url_for('static', filename='img/about/icon-laura.jpg') }}" alt="Card image cap">
							<h4 class="card-title">Laura Catano</h4>
							<p class="card-text">
								Bio: I'm a third year CS student. I play 
								Overwatch and like designing 3D computer 
								graphics
							</p>
							<p class="card-text">
								Responsibilites: Maintains about page, Data Scraping/Parsing.
							</p>
							<p class="card-text">Commits: 33</p>
							<p class="card-text">Issues: 6</p>
							<p id ="cardbottom" class="card-text">Unit Tests:0</p>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-md-3">
					<div class="card">
						<div class="card-block">
							<img class="card-img-top img-responsive" src="{{ url_for('static', filename='img/about/Conner.jpg') }}" alt="Card image cap">
							<h4 class="card-title">Conner Newman</h4>
							<p class="card-text">
								Bio: I'm a second year CS student. My favorite 
								programming languages are Python, Java, and C++.
								 I like reading, watching TV, and playing 
								video games.
							</p>
							<p class="card-text">
								Title: Project Manager and Senior Database Administrator
								Responsibilites: API scraping, SQLAlchemy database populating
							</p>
							<p class="card-text">Commits: 53</p>
							<p class="card-text">Issues: 14</p>
							<p id ="cardbottom" class="card-text">Unit Tests:4</p>
						</div>
					</div>
				</div>
			</div> 		
		</div> 	
		<div class="container">
			<div class="row">
				<div class="col-md-6">
					<h2>Stats</h2>
					<p>Total Commits: 225</p>
					<p>Total Issues: 30</p>
					<p>Total Unit Tests: 10</p>
					<p>Apiary URL: <a href="http://docs.swereaders1.apiary.io">http://docs.swereaders1.apiary.io</a></p>
					<p>Github Issue Tracker: <a href="https://github.com/DasNando/idb/issues">https://github.com/DasNando/idb/issues</a></p>
					<p>Github Repo: <a href="https://github.com/DasNando/idb">https://github.com/DasNando/idb</a></p>
					<p>Presentation Link: <a href="presentation.link">https://github.com/DasNando/idb</a></p>
				</div>
				<div class="col-sm-8 col-md-3">
					<h2>Data Sources</h2>
					<p><a href="https://openlibrary.org/developers/api">OpenLibrary</a></p>
					<p><a href="http://www.goodreads.com/api">Goodreads</a></p>
					<p><a href="https://developers.google.com/books/docs/v1/using">Google Books</a></p>
				</div>
				<div class="col-sm-4 col-md-3">
					<br>
					<a class="btn btn-runtest btn-block" href="https://travis-ci.org/DasNando/idb">Run Tests</a>
				</div>
			</div>
		</div>
		<br>
		<div id="techreport" class="container">
			<h3>CS373 Project 4 - IDB2</h3>
			<h2>Technical Report</h2>
			<h3>Introduction</h3>
				<p>
					While the world may have moved into a new digital era, the printed word has been and continues 
					to be a relevant and readily accessible source of information and entertainment. 
					Our project is a database of books, publishers, authors, and 
					book reviews to be built from data scraped from various 
					available web APIs on this subject (Google Books, 
					GoodReads, Wikipedia). Our goal is simply to provide an alternative to 
					services like GoodReads and Google Books and to make the world's extensive collection of literature even more accessible.
				</p>
				<p>
					We aim to provide the means by which users may browse books, authors, publishers, and reviews
					and filter or sort them as desired. Some specific use cases: If you need to find affordable textbooks,
					 you can sort books by price. If you want to discover new authors in a genre you like, you can filter authors by genres 
					 they have written in. If you would like to find publishers in your country you can do that too. Our site is intended for use by anyone who may find such a catalog of book related information to be useful, i.e libraries, students shopping for textbooks, bookstores, or 
					people who would like to get book recommendations based on genres and authors they may like.

				</p>
			<h3>Design</h3>
				<h4>RESTful API</h4>
				<p>
					Our RESTful API allows others to scrape our database for the 
					information we have collected. Our API will allow users to filter the database by 
					search terms such as book name, author name, publishing date, 
					etc. It will also allow you to pull all information on a given 
					specific book, or get a long list of all books, authors, publishers or reviews. This API
					was created and hosted on Apiary, and can be accessed by anyone by the usual means of making RESTful 
					API GET requests.
				</p>

				<p>
					The data in our database are accessible via a number of user-friendly API calls. For each of our models, there is a set of API calls associated with that model. Firstly, there is one API call for each model which returns a JSON object containing all of the data from our database for that model. It would obviously become problematic to create and return such a list if the size of our database grew significantly larger, but currently our data set is small enough that it is able to return all of the data for any particular model without difficulty. These calls are accessed via the following URLs:
				

				<h5>swe-readers.me/books/params&{parameters} [GET]</h5>

				<h5>swe-readers.me/books/title={book_name} [GET]</h5>
				<h5>swe-readers.me/book [GET]</h5>

				<h5>swe-readers.me/authors?id={author_id} [GET]</h5>

				<h5>swe-readers.me/publishers/searchterm={terms} [GET]</h5>
				<h5>swe-readers.me/publisher?id={publisher_id}  [GET]</h5>

				<h5>swe-readers.me/reviews/searchterm={terms}  [GET]</h5>
				<h5>swe-readers.me/review?id={review_id}  [GET]</h5>

				<p>
					The /api/books/all/ URL, for instance, returns a JSON list of dictionaries, where each dictionary in the list represents one book. This dictionary then contains key-value pairs for each of the book model's attributes. Thus the final payload takes the form of:
				</p>

				<pre id="techreportpre"><code><!--... -->
					[
						{
						    "author": "Maya Angelou", 
						    "genre": "Biography & Autobiography", 
						    "isbn": "030747772X", 
						    "pic": "http://books.google.com/books/content?id=Z2q_1A0nlvIC&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api", 
						    "prices": "This title is not for sale.", 
						    "publisher": "Random House", 
						    "rating": "4.0", 
						    "title": "I Know Why the Caged Bird Sings", 
						    "year": "2010-07-21"
						  },
						  ...
					  ]
				</code></pre>

				<p>
					On the back end, these data are collated and served by the main.py script (with a little help from Flask). For instance, the back end code got the /api/books/all/ API call looks like this:

				</p>

				<pre id="techreportpre"><code><!--... -->
						# get all books
						@app.route('/api/books/all/')
						def get_book0():
						    b_dict_list = []

						    book = models.Book.query.all()
						    for b in book:
						        b_dict_list.append(
						            {"title": b.title, "genre": b.genre, "year": b.year, "isbn": b.isbn, "prices": b.prices, "pic": b.pic,
						             "author": b.author_name, "publisher": b.publisher_name, "rating": b.rating})
						    # print book.title models.Book.query.limit(lim).all()
	                        return jsonify(b_dict_list)
	
				</code></pre>

				<p>
					First, Flask handles the routing of the URL to the proper function. This means that every time a user tries to load swe-readers.me/api/books/all/, Flask causes the get_book0 function to be invoked. The function then creates an empty list, and, for every 'book' entry in the database, creates and appends to this list a dictionary containing that book's title, genre, date published, ISBN, price, cover picture, author, publisher, and average rating. Having iterated over all of the books in the database, the function then returns a JSON representation of the list it has created.
				</p>

				<p>
					If, instead of getting a rather verbose and messy list of all of the data in our database for a particular model, a user wants to get a subset of the data, our website also supports API calls to do this. The "/api/books/title=<string:book_name>" works similarly to the "/api/books/all" call, but only returns data for books whose titles match the given string. On the back end, this is accomplished by a database query which only returns those books with book_name in their title. In the code, this is done with:
				</p>

				<pre id="techreportpre"><code><!--... -->
						models.Book.query.filter(models.Book.title.ilike(book_name)).all()

				</code></pre>

				<p>
					It is also possible, via the "/api/books/params&<string:params>" call, to perform a query for all books matching an arbitrary set of parameters. This function parses the parameter string passed into it and attempts to match arguments to columns in the Book table. It then searches the database for entries matching the provided parameters. So, a call to the URL "http://swe-readers.me/api/books/params&author_name=an" will return a list containing every book with an author whose name contains the string "an" (e.g. Anne Lamott, Jennifer Ackerman, Maya Angelou, etc.). This can be further refined with a call like: "http://swe-readers.me/api/books/params&author_name=an&title=cag", which can be used to obtain only those books with authors whose names include "an" and whose titles include "cag". In our database, this returns only:
				</p>


				<pre id="techreportpre"><code><!--... -->
							[
							  {
							    "author": "Maya Angelou", 
							    "genre": "Biography & Autobiography", 
							    "isbn": "030747772X", 
							    "pic": "http://books.google.com/books/content?id=Z2q_1A0nlvIC&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api", 
							    "prices": "This title is not for sale.", 
							    "publisher": "Random House", 
							    "rating": "4.0", 
							    "title": "I Know Why the Caged Bird Sings", 
							    "year": "2010-07-21"
							  }
							]

				</code></pre>

				<p>
					which is a list containing only one entry, that for Maya Angelou's "I Know Why the Caged Bird Sings".
                    In addition to author name and title, book searches can be done by genre, ISBN, price, publisher, rating, and date published. These parameters can all be searched either individually or in combination with one another.
                    In this way, our API enables the querying of any model by any of that model's attributes. These URLs can then be queried via an HTTP GET in order to retrieve data from our database in JSON form.
				</p>



				<h4>Frontend</h4>
				<p> Our site's front-end is made with HTML and Bootstrap. 
					In Phase 1, it was simply a splash page (with a carousel and tables 
					of books), pages (containing grids corresponding to each of 
					our four models), and a currently non-functioning search bar. Instead of the splash
					image page, at this stage we opted for the comparatively less slick home page, featuring a random 
					sampling of books in the carousel as well as columns for high rated and recent books. We felt that the
					IMDB/ old school archive look matched our topic a little better.
					
				</p>
				    In Phase 2 we used Angular/Javascript to improve the look and 
					navigability of our site. We added pagination to our site in this phase, 
					allowing users to browse pages containing members of each model. We also added 
					filtering and sorting, so that users would be able to filter or sort models by 
					their specific attribute. As an example, by this time we could now sort books by Year, 
					filter books by a specific author, and flip through the results in pages of about 10 entries 
					each without leaving the /books path. Each entry of the grid was represented by a card with fields 
					for the models attributes filled in from our database.

				</p>	  
				<p> 
	                As of Phase 3, we have implemented search, which allows users to search all attributes of all models 
				    all at once. We have also implemented the models' details pages, which can be reached by clicking through an
				    entry on the model browsing pages implemented in Phase 2. These detail pages contain more specific information about their individual instance of a book, author, publisher, or review and are separate from all of the other model instances, while retaining the same HTML template. During this phase we also went back and added the model to model links to our site, which we meant to implement in Phase 2, but were unable to due to unforseen issues with our database. 
				</p>
				<p> 
					As for how our frontend displays data at the current moment, as per project specs, our website is divided into several sub-pages, one for each model. Each model has its own page where the database entries for that model are displayed in a grid format. Our site employs Bootstrap in order to dynamically create this grid format. This grid is paginated and each page is limited to 24 entries. This pagination is implemented using a script called 'dirPagination.js', which allows us to create dynamically filled grid elements. This script has the same functionally as the Angular repeat function (ng-repeat) with the added benefit of functional pagination and of dynamically creating pages based on the number of elements per page and the total number of elements in the grid. The two use the same filters and ordering statements, allowing for seamless traversal of the pages of the grid.

                    Our app.js script then queries our API to get the JSON for each model (using the /api/<model_name>/all/ URL), and creates a list of books from this data. This list is then used by the dir-paginate.js script to dynamically insert the data into the HTML.
				</p>
				<h4>Backend</h4>
				<p>
					To serve our webpages, we are using the Flask Python web 
					framework. Flask allows us to render HTML templates and use 
					Python to apply logic to the rendering and change the website
					accordingly. As of now, the system we are using is used 
					primarily to keep track of all static files, such as 
					stylesheets and images, and apply them properly to the 
					html pages. As the project develops, we'd like to reduce 
					the number of page templates and increase the amount of media 
					that we maintain. As of Phase 1, the Flask framework did not 
					serve any static images, as they were being pulled from 
					the web. Eventually, we would like to add some images such 
					as a logo or a static carousel.

				</p>
					During Phases 2 and 3 we needed to create routes for more specific model
					 details pages which displayed all the surface attributes of a model plus more
					  verbose data we are unable to display in the browsing page cards. Doing 
					  this required us to build more HTML templates and serve the data through JavaScript. 
					  This led to some problems that we will discuss later. During these Phases we also tinkered 
					  with some different ways of organizing our models, but ended up keeping the older, more 
					  simpler organization.
				<p>


				</p>

				<h4>Media Embedding Services</h4>
				<p> 
					As of now, we have not used any media embedding services, 
					but we plan on eventually embedding verbose links to reviews 
					via GoodReads. This means including some small portion of a 
					larger review or set of reviews and attaching them to each 
					Book, Author, and Publisher. The implementation of this and 
					which service we will use is yet to be determined. We are 
					also considering using a dynamic embedding service to update 
					our carousel according to changes in popularity, or simply 
					to randomize the images posted.
				</p>
				<h4>Database</h4>
				<h5>Attributes</h5>
				<p>
					For this part of the project, we needed to implement the database that was set up for phase 1. Previously, we had just had a skeleton for a database, but without anything filling it. In actually implementing the database and populating it with the data scraped from our API sources, we discovered a number of challenges. Some of the attributes from our models turned out to be difficult or otherwise problematic to implement. For the version of the database implemented in this version of the project, we decided to change the following attributes:

				</p>
				<p>Book:</p>
				<p>
					-The attribute 'year' was changed from a column Integers to a  column of Strings
					-The attribute 'edition' was removed entirely
					-The attribute 'prices' was changed from a column of Floats to a column of Strings
					-A new attribute called 'pic' was added, storing a string representing the url of an image for the book
				</p>
                <p>Author:</p>
				<p>
					-The attribute 'alive' was removed entirely
					-The attribute 'works' was removed entirely
					-The attribute 'genre' was removed entirely
					-A new attribute called 'pic' was added, which stores a string representing the url of an image for the author
					-A new attribute called 'about' was added, of type 'Text', which stores a short bio paragraph for the author
					-A new attribute called 'num_works' was added, storing as a string the number of published works by that author
				</p>
				   <p>Publisher:</p>
				<p>
					-The attribute 'founders' was removed entirely
					-A new attribute called 'about' was added, of type 'Text', which stores a short bio paragraph for the publisher
				</p>
				   <p>Review:</p>
				<p>
					-The attribute 'rating' was changed from a column of Floats to a column of Strings
				</p>
				<h5>Changes</h5>
				<p>
					These changes were implemented as we began to write and test our scripts for API scraping and database populating. On of the significant changes to notice is that, whereas our database had been previously set up to store a mixture of String and Number data types, it now stores only text representations of data. This approach yields a number of advantages over storing raw number types in a database. First, it allows differentiation between existent and non-existent data values. For instance, one of the problems that we ran into in attempting to populate the database with information on our books was that, while the Google Books API returned a JSON payload containing pricing information on most of the books, for some of the books in our search this data was mysteriously absent. Further investigation showed that this was due to the fact that, while some books were listed as “For Sale”, others were not, and thus lacked a “retailPrice” attribute in their JSON representation. If we still wanted to store this ‘prices’ attribute as a float type, this would pose a problem. We could store it as a price of ‘0’, but that would be confusing and not particularly helpful or descriptive to the end user looking at out data. On the other hand, storing this data as strings allows us to handle this potential problem. Our code is now able to check whether a given book has an associated price. If a book does have a price, then the book entry is simply created using the string representation of that price, but if the book does not have an associated price, then a book entry is created where the ‘prices’ attribute stores the string 'This title is not for sale.' This implementation allows us to store the prices of books but also to provide useful information to the end user in a convenient manner without additional work. On the back end, the pricing information can be generated once and stored in statically in the database. On the front end, the web page needs only to load the string value stored in the ‘prices’ field and display it, and no additional dynamic work is required on page load. 
					Additionally, there were a number of fields which were removed and replaced with other information. This was done if and when we discovered that the APIs which we used to source our data did not contain the data that we had originally hoped to use.
				</p>



				<h4>Data Sources</h4>
				<p>
					To collect our data we used the GoogleBooks, GoodReads, and Wikipedia APIs. 
					Some of these APIs return xml responses, some return json responses and some have strict
					limits on how many requests can be made and how their data can be used but they are all
					free to use. To scrape this data we begin with a list of keywords to request from the APIs.
					 These keywords can be ISBNs, IDs internal to the API, or names of publishers. 
					 We store the responses in a .json array.
				</p>

				<p>
					Our data were scraped programmatically, using a number of python scripts. To scrape our data, we started first with a script which used the Google Books API to get information on a number of books. To do this, our script simply had the following logic:
				</p>

				<pre id="techreportpre"><code><!--... -->
subjects = ['technology', 'sports', 'literature', 'poetry', 'romance', 'mystery', 'science+fiction']
for i in subjects:
	url = "https://www.googleapis.com/books/v1/volumes?q=subject:"+ str(i) + "&maxResults=40&langRestrict=en&key=AIzaSyBIbqz2Qb-Afi-v1iXP3QRdM8wQmQMdFG4"
				</code></pre>

				<p>
					This loop, thus, took a list of seven different book subjects, and for each subject, got a list of 40 different books. Having stored these 280 data points in a JSON file, we could use this information to scrape information about these books’ authors and publishers. To generate the information on authors and publishers, our code iterated over the JSON file for the books, grabbed the Author and Publisher attributes from each entry, and compiled a list of author names and a list of publisher names. From these lists, we scraped the Goodreads API as such:
				</p>

				<pre id="techreportpre"><code>
author_names = ['Christina Dodd', 'Simone Murray',  'Dante Alighieri',  

for author in author_names:
	url1 = 'https://www.goodreads.com/api/author_url/' + urllib.parse.quote(author, safe='') + '?key=YCBYRWAFTg2cfUnmW8IWTg'
			    </code></pre>
			    <p>
					to generate a JSON formatted description of our authors, and:
				</p>

				<pre id="techreportpre"><code>
pubs = ["Harper Collins", "Simon & Schuster", "Random House", …

for i in pubs:
	url = "https://en.wikipedia.org/w/api.php?action=query&format=json&formatversion=2&prop=revisions&titles=" + urllib.parse.quote(i, safe='') + "&redirects=1&rvprop=content&rvparse=1" 
				</code></pre>
				<p>
					to generate a JSON formatted description of our publishers.
					After this is completed, our scripts have generated three separate JSON files, one each for the models Books, Authors, and Publishers. This implementation also conveniently means that we will automatically have information on authors and publishers that are related to the book data that we source (i.e. this setup gets information on a book x and then automatically also generates information for x’s author and publisher.
				</p>
				<p>
					We used the Google Books API as the source for data about 
					individual books, the GoodReads API as the source for data about authors
					and book reviews and the Wikipedia API for general data about publishers.
					We store the data from the responses in .json format in order to 
					more easily be able to rebuild the cache when necessary. Responses
					not in a .json format as is necessary to be able to very easily and painlessly read data into the 
					database are parsed, have their salient data extracted, and are then converted 
					into a .json format.
				</p>
				<p>
					Having scraped these data, we need a script to add them to the database. This was accomplished using a single script (setup_db.py). This script starts by first loading all of the JSON files for the models. Then, for each model, the script iterates over the JSON data for that model’s file, generating one instance of the relative model per JSON entry. This script takes special care to make sure that for every element it attempts to retrieve from the JSON, a valid dictionary entry exists for that key. This is important because it is entirely possible for some models that the API call could have returned a JSON payload which was only partially complete. For instance, if the script is unable to determine a genre for a given book, it assigns the string 'error fetching genre' to that model’s instance for the ‘genre’ attribute. Having successfully created a Model instance, the script then queries the database to guarantee that the database does not already contain a conflicting entry (i.e. no two entries can contain the same primary key, so the script cannot add two books with the same title), and if not, adds the model instance to the database.
				</p>
			<h4>Models</h4>
				<p>The models each have a number of sub-attributes as follows:</p>
				<img class="img-responsive" src="{{ url_for('static', filename='img/about/IDB2.png') }}" alt="uml diagram">
				<p>
					Each model is supported by a SQLAlchemy table which holds 
					all of that model's data. The table stores all of the data 
					for that model with each column of the table representing 
					one attribute of the model. Currently, the 'book' model 
					has title as its primary key, while 'author', 'review', 
					and 'publisher' each have the 'name' attribute as their 
					primary key.
				</p>
				<p>
					Each model has links to other models. Each book is linked, 
					via one-to-one linkings, with its author and its publisher, 
					and with a one-to-many with reviews of that book. Each author 
					has a one-to-many relationship with that author's books and a 
					one-to-one relationship with that author's publisher. 
					Publishers have a one-to-many relationship with authors and 
					with books. Lastly, Reviews have one-to-one relationships 
					with both authors and books.
				</p>
				<p>
					Each model is represented by a separate class, all of which 
					extend the db.Model class. Upon instantiation, each model's 
					__init__ method checks, via the use of assert statements, 
					that any string data members have been set to values with 
					positive length, as well as ensuring that reviews and prices 
					are given as non-negative numbers. This is done to ensure the 
					validity of any new data members that are created.
				</p>
			<h3>Tools</h3>
				<h4>SQLALchemy</h4>
				<p>
					SQLAlchemy is a Python Flask extension and object relational mapper 
					that provides us with a toolkit for more easily working with SQL databases. 
					It is used to create models, columns, and tables, and input our scraped 
					data into our database. 
				</p>
				<h4>Flask</h4>
				<p>
					Flask is a microframework for Python, using the Jinja2 and Werkzeug WSGI 
					libraries. It allows us to map out routes from HTML templates to URL addresses 
					within our site, useful for implementing our sites navigation. Additionally, Flask provides support for API querying, and the Flask-SQLAlchemy extension, which was invaluable for us in allowing us to create and set up our models within the database.
				</p>
				<h4>PostgreSQL</h4>
				<p>
					PostgresSQL is an open source object-relational database system and server that we used to
					host our database. It has a Python interface that allows us to freely manage our database.
				</p>
				<h4>Slack</h4>
				<p> Our team communicates using Slack. Slack allows us to create group channels, each with a particular purpose, such as development or random chat. This allows free communication between teammates while maintaining structure, such that no thread ever goes off topic. Slack also allows us to use the Github integration plugin, which keeps team members up to date with commits and pushes made to the joint Github repository. This allows all team members to receive the same information and reduces confusion among group members.
				</p>
				<h4>Bootstrap</h4>
				<p>
					Bootstrap is a free, open source frontend web framework that provides us with HTML 
					and CSS templates, like buttons, cards, and carousels to use in our site's design 
					and navigation. 
				</p>
				<h4>AngularJS</h4>
				<p>
				 AngularJS is a JavaScript based, open source, frontend web framework specifically designed for working with single page applications. We used it to implement sorting, filtering and pagination on our site, and to bind relevant data to the pages served for a particular instance of a model.
				</p>
				<h4>HTML</h4>
				<p>
					HTML is a markup language for semantically specifying the structure and content of web pages.
					 We used it to create static templates for each of our models pages ad well as for our individual review pages. Its also the basis for our splash and about pages.
				</p>
				<h4>PlanItPoker</h4>
				<p>
					A Scrum tool by CodeFirst for laying out user stories and voting on estimates for each story as a group. It allows the the group to vote together on story points and in the process agree on the complexities of the task of implementation, so each member of the group can be on the same page. We used it to plan out and estimate the tasks for Phase 3.

				</p>
				<h4>D3.js</h4>
				<p>
					D3 is a JavaScript library for manipulating documents' data and provides visualization tools for creating graphics based on the data, with support for various popular JavaScript graphics libraries. It allows efficient manipulation of the Document Object Model. We used it to create a visualization of our assigned group's data.
				</p>
				<h4>Github</h4>
				<p>
					Github is a remote repository service and versioning tool that allowed our
					 group to keep the current version of our code neatly in one place during 
					 development, making more efficient work management and code sharing possible. 
				
				</p>
				<h4>Google Cloud Platform</h4>
				<p>
					Google Cloud Platform is a cloud computing service created by Google on which we hosted our website. Although it is a paid service, we 
					were allowed free access for the purposes of this project. Google Cloud Platform's App Engine is the basis for our site, providing us with PostGreSQL database support and integrating our changes on Github into the website automatically. 
				</p>
				<h4>PyLint</h4>
				<p>
					PyLint is a linting tool for Python, based on the PEP 8 style. We used this 
					tool to ensure that all our code is always sensible and well formed.
				</p>
				<h4>AutoPEP8</h4>
				<p>
					AutoPEP8 is a tool for automatically restructuring code to the PEP 8 style, we used it to 
					maintain a consistent codebase.
				</p>
				<h4>Python</h4>
				<p>
					Python is an interpreted, high-level programming language, created by Guido van Rossum. It uses very readable English-like syntax and supports imperative, procedural, object-oriented and functional programming styles. Python is the very core of our application, we use it to scrape date, build our models, and interface with the database.
				</p>
				<h4>yUML</h4>
				<p>
					yUML is an online tool for creating UML diagrams, by specifying models,
					 attributes and their relationships in a text file. This let our group 
					 quickly make diagrams without needing to learn any new technologies in the process.
				</p>
				<h4>BeautifulSoup</h4>
				<p>
					BeautifulSoup is a python parsing library designed for quick and easy XML and 
					HTML scraping. It provides a more intuitive wrapper over common python parsing 
					libraries such as lxml and html5lib and allows us to create cleaner and more elegantly written
					 parsing code. We used this library to parse HTML returned from requests to the 
					 Wikipedia API.
				</p>
				<h4>Apiary</h4>
				<p>
					Apiary is an API integration cloud provided by Oracle, that provides a 
					framework and tools for building and sharing APIs. We used it to setup our SWEreaders
					 API.
				</p>
				<h4>Travis CI</h4>
				<p>
					For this project's deployment testing, we are using TravisCI. TravisCI allows us to automate the running of unit tests on each push to the group Github repository. These tests allow the team to ensure that changes made are not unknowingly interfering with other sections of the project, minimizing the amount aof site-crashing bugs that make it to live. The unit tests will largely consist of tests on the output of queries to the database/API. As of now, there is little to run on Travis, but as the project develops, the amount of tests run by Travis should increase.
				</p>
			<h3>Hosting</h3>
				<p>Our website is hosted on Google Cloud Platform's App Engine. 
					Google Cloud Platform allows us to maintain source code on 
					Google's servers and sync with Github in order to streamline 
					updates to the website. As of now, our App Engine only hosts 
					a single machine which, using the Flask Python web framework, 
					serves the HTML templates of the website and applies static 
					files such as CSS files and images. Our website's error logs 
					are also maintained by Google, and include logs of all requests 
					made to the website, filterable by severity of the request
					(ie. No Error, Warning, Notice, etc.)
				</p>
				<p>
					Normally setting up an application on GCP would require paying for the service but, we were luckily able 
					to bypass this and set up for free for the purposes of the class. 
				</p>
			<h3>Pitfalls and Learning Opportunities</h3>
			<p>
				Throughout SWE-Readers' development process we ran into a lot of problems with our site and database. In our backend, we had issues correctly setting up many to many relationships which were never fully resolved due to time constraints. We set up our association tables but were never able to add our data to them. In our hosting, we later ran into issues querying our database appropriately while trying to serve it on the SWE-readers site, which led to us not being able to implement this feature during Phase 2. We were able to resolve this issue in the final phase, which, as it turned out, was due to GCP not supporting external outgoing connections to our PostgreSQL database which was hosted separately from Google Cloud. In the end we ended up having to recreate our site and database to obtain the appropriate permissions. GCP's Postgres support is still in beta, which was both a blessing and a curse for us, since it led to unexpected problems but was still better than no support at all. In data scraping we ran into issues with parsing, leading to occasionally poorly formatted text in author biographies and publisher data, issues which could honestly have been easily resolved with more time to work on them. We also had minor issues with incomplete and non specific data such as not all publishers having the locations of their headquarters listed on Wikipedia, which isn't really our fault and many of our book entries being of the non specific genre 'Fiction' which is not very helpful to a theoretical user who would like to find books by genre and some data returned for a small handful books occasionally lacking an ISBN. Some of these issues could have been corrected by cross referencing our scraped data with another API, such as OpenLibrary, but that would have taken up too much time we did not have.
			</p>
			<h3>PlanIt Poker</h3>
			<p>
				Create Visualization, Estimated: 3 hours, Actual: Not done yet <br> 
				Create Self Critique, Estimated: 1 hour, Actual: Not done yet <br>
				Create Other Group Critique, Estimated: 1 hour, Actual: Not done yet <br> 
				Create Presentation, Estimated: 3 hours, Actual: Not done yet <br>
				Update About Page, Estimated: 20 minutes, Actual: 25 minutes <br> 
				Update Technical Report, Estimated: 6 hours, Actual: 4 hours <br>
				Scrape more reviews, Estimated: 10 minutes, Actual: 1 hour <br>
				Implement model specific details pages, Estimated: 3 hours, Actual: 2 hours <br>
				Implement search function, Estimated: 3 hours, Actual: 4 hours <br>
				Fix problems with database, Estimated: 1 hour, Actual: 9 hours
		   </p>
	</div>
		<br>
	</body>
	<footer>
		<h6><small>A UTCS Student Project. CS373 Software Engineering, Project Group 15.</small></h6>
	</footer>
</html>